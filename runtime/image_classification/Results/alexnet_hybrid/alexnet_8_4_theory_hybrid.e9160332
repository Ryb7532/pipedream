Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 233, in main
    macrobatch=args.macrobatch)
  File "../sgd.py", line 23, in __init__
    macrobatch=macrobatch,
  File "../optimizer.py", line 41, in __init__
    master_parameters, **optimizer_args)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/optim/sgd.py", line 64, in __init__
    super(SGD, self).__init__(params, defaults)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/optim/optimizer.py", line 45, in __init__
    raise ValueError("optimizer got an empty parameter list")
ValueError: optimizer got an empty parameter list
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 233, in main
    macrobatch=args.macrobatch)
  File "../sgd.py", line 23, in __init__
    macrobatch=macrobatch,
  File "../optimizer.py", line 41, in __init__
    master_parameters, **optimizer_args)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/optim/sgd.py", line 64, in __init__
    super(SGD, self).__init__(params, defaults)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/optim/optimizer.py", line 45, in __init__
    raise ValueError("optimizer got an empty parameter list")
ValueError: optimizer got an empty parameter list
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:4501

Exception in thread Thread-2:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:28346

Exception in thread Thread-3:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:56489

Exception in thread Thread-44:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:22245

Exception in thread Thread-44:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:27555

Exception in thread Thread-44:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:2189

Exception in thread Thread-44:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:32402

Exception in thread Thread-44:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:11674

Exception in thread Thread-44:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:18821

Exception in thread Thread-44:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:29070

Exception in thread Thread-2:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:47495

Exception in thread Thread-5:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 488, in recv_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 536, in _recv
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:31441

Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/6/17B13541/pipedream/runtime/image_classification/launch.py", line 173, in <module>
    main()
  File "/home/6/17B13541/pipedream/runtime/image_classification/launch.py", line 169, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/bin/python3', '-u', 'main_with_runtime.py', '--rank=31', '--local_rank=3', '-s', '--distributed_backend', 'gloo', '-m', 'models.alexnet.gpus=8_4_theory', '--epochs', '6', '-b', '256', '--config_path', 'models/alexnet/gpus=8_4_theory/hybrid_conf.json', '--num_ranks_in_server', '4', '--master_addr', '10.0.3.85']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    main()
  File "main_with_runtime.py", line 330, in main
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    train(train_loader, r, optimizer, epoch)
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    r.run_forward()
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
  File "../runtime.py", line 510, in run_forward
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    main()
  File "main_with_runtime.py", line 330, in main
    for input_name in input_names])
    train(train_loader, r, optimizer, epoch)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
  File "main_with_runtime.py", line 392, in train
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
    result = self.forward(*input, **kwargs)
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 24, in forward
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    out4 = self.layer4(out3)
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    out5 = self.layer5(out4)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    main()
  File "main_with_runtime.py", line 330, in main
    result = self.forward(*input, **kwargs)
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 339, in forward
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 1; 15.90 GiB total capacity; 14.71 GiB already allocated; 17.38 MiB free; 493.13 MiB cached)
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 2; 15.90 GiB total capacity; 14.71 GiB already allocated; 19.38 MiB free; 493.13 MiB cached)
    self.padding, self.dilation, self.groups)
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 3; 15.90 GiB total capacity; 14.71 GiB already allocated; 79.38 MiB free; 493.13 MiB cached)
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 3.38 MiB free; 493.13 MiB cached)
RuntimeError: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 47.38 MiB free; 409.25 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 24, in forward
    out5 = self.layer5(out4)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 339, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 2; 15.90 GiB total capacity; 14.75 GiB already allocated; 115.38 MiB free; 409.25 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 51.31 MiB free; 493.13 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 2; 15.90 GiB total capacity; 14.71 GiB already allocated; 53.31 MiB free; 493.13 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 3; 15.90 GiB total capacity; 14.71 GiB already allocated; 79.38 MiB free; 493.13 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 1; 15.90 GiB total capacity; 14.71 GiB already allocated; 79.38 MiB free; 493.13 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    main()
  File "main_with_runtime.py", line 330, in main
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 24, in forward
    out5 = self.layer5(out4)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 339, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 24, in forward
    out5 = self.layer5(out4)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 3; 15.90 GiB total capacity; 14.79 GiB already allocated; 5.31 MiB free; 470.00 MiB cached)
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 339, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 1; 15.90 GiB total capacity; 14.75 GiB already allocated; 51.25 MiB free; 444.25 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 24, in forward
    out5 = self.layer5(out4)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 339, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 3.38 MiB free; 444.25 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 15.38 MiB free; 493.13 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 24, in forward
    out5 = self.layer5(out4)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 339, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 1; 15.90 GiB total capacity; 14.79 GiB already allocated; 13.38 MiB free; 470.00 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    main()
  File "main_with_runtime.py", line 330, in main
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 79.38 MiB free; 493.13 MiB cached)
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 1; 15.90 GiB total capacity; 14.71 GiB already allocated; 11.38 MiB free; 493.13 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 3; 15.90 GiB total capacity; 14.66 GiB already allocated; 79.38 MiB free; 535.38 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 2; 15.90 GiB total capacity; 14.71 GiB already allocated; 11.38 MiB free; 493.13 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 3; 15.90 GiB total capacity; 14.71 GiB already allocated; 59.31 MiB free; 493.13 MiB cached)
Traceback (most recent call last):
  File "main_with_runtime.py", line 623, in <module>
    main()
  File "main_with_runtime.py", line 330, in main
    train(train_loader, r, optimizer, epoch)
  File "main_with_runtime.py", line 392, in train
    r.run_forward()
  File "../runtime.py", line 510, in run_forward
    self._run_forward(tensors)
  File "../runtime.py", line 558, in _run_forward
    for input_name in input_names])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 368, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/pipedream/runtime/image_classification/models/alexnet/gpus=8_4_theory/stage0.py", line 23, in forward
    out4 = self.layer4(out3)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 507, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/modules/pooling.py", line 146, in forward
    self.return_indices)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/_jit_internal.py", line 133, in fn
    return if_false(*args, **kwargs)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 490, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 482, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 2; 15.90 GiB total capacity; 14.67 GiB already allocated; 79.38 MiB free; 534.88 MiB cached)
Exception in thread Thread-43:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:43427
Exception in thread Thread-66:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:34737


Exception in thread Thread-66:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:32168
Exception in thread Thread-43:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:62344


Exception in thread Thread-43:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:21894

Exception in thread Thread-66:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:5046

Exception in thread Thread-43:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:3787

Exception in thread Thread-66:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:60111

Exception in thread Thread-43:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:32406
Exception in thread Thread-66:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:56971


Exception in thread Thread-43:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:27842
Exception in thread Thread-66:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:37528


Exception in thread Thread-66:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:37799

Exception in thread Thread-43:
Traceback (most recent call last):
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/apps/t3/sles12sp4/free/python/3.6.5/gcc4.8.5/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "../communication.py", line 511, in send_helper_thread
    sub_process_group=sub_process_group)
  File "../communication.py", line 579, in _send
    group=sub_process_group)
  File "/home/6/17B13541/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 755, in broadcast
    work.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [10.0.6.85]:63404

